{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import scipy\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['college name', 'apps received', 'apps accepted', 'new stud enrolled', '% new stud from top 10%', '% new stud from top 25%', 'num FT undergrad', 'num PT undergrad', 'in-state tuition', 'out-of-state tuition', 'room', 'board', 'add fees', 'est book costs', 'est personal costs', '% fac with PHD', 'stud:fac ratio', 'graduation rate']\n"
     ]
    }
   ],
   "source": [
    "data=csv.reader(open('/Users/wendy/Documents/2017 Fall/CS 534/HW5/Colleges.txt'))\n",
    "data_list=[]\n",
    "for row in data:\n",
    "    data_list.append(row)\n",
    "features=data_list[0][0].split('\\t')\n",
    "data_list.pop(0)\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######not important, ignore this cell######\n",
    "whole_data=[]\n",
    "university_name=[]\n",
    "for i in range(len(data_list)):\n",
    "    whole_data.append(data_list[i][0].split('\\t'))\n",
    "    for j in range(1,len(whole_data[i])):\n",
    "        if whole_data[i][j]!=\"\":\n",
    "            whole_data[i][j]=float(whole_data[i][j])\n",
    "    university_name.append(whole_data[i][0])\n",
    "    whole_data[i][0]=i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use mean to replace missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2752.0975232198148, 2: 1870.6831913245549, 3: 778.88049344641468, 4: 25.671977507029148, 5: 52.349999999999895, 6: 3692.6651270207858, 7: 1081.5267716535427, 8: 7897.2743710691793, 9: 9276.9056162246452, 10: 2514.6819571865472, 11: 2060.9838308457724, 12: 392.01264591439633, 13: 549.97288676236019, 14: 1389.291703835858, 15: 68.645669291338578, 16: 14.858769230769228, 17: 60.405315614617876}\n"
     ]
    }
   ],
   "source": [
    "dic={}\n",
    "for i in range(1,len(whole_data[1])):\n",
    "    s=0\n",
    "    counts=0\n",
    "    for j in range(len(whole_data)):\n",
    "        if whole_data[j][i]!='':\n",
    "            s+=whole_data[j][i]\n",
    "            counts+=1\n",
    "            dic[i]=s/counts\n",
    "print dic\n",
    "    \n",
    "for i in range(1,len(whole_data[1])):\n",
    "    for j in range(len(whole_data)):\n",
    "        if whole_data[j][i]=='':\n",
    "            whole_data[j][i]=dic[i] \n",
    "            \n",
    "whole_data=np.array(whole_data)\n",
    "whole_data_scaled = preprocessing.normalize(whole_data[:,1:18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the major components,total variance and square_err for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_variance(data):\n",
    "    for i in range(1,len(data[1])):\n",
    "        pca = PCA(n_components=i)\n",
    "        p=pca.fit_transform(data)\n",
    "        s=sum(pca.explained_variance_ratio_)\n",
    "        ori_pca=pca.inverse_transform(p)\n",
    "        square_error=metrics.mean_squared_error(data,ori_pca)\n",
    "        if s>0.95:\n",
    "            break\n",
    "    print \"the major components are :\",pca.explained_variance_ratio_\n",
    "    print \"total variance is:\",s\n",
    "    #print \"singular values are:\",sing\n",
    "    print \"square error for pca is\", square_error\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare normalized data and unnormalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the major components are : [ 0.7431399   0.09230213  0.05211807  0.03428852  0.02876712]\n",
      "total variance is: 0.950615726993\n",
      "square error for pca is 0.000546298296426\n",
      "the major components are : [ 0.56627776  0.3470008   0.03455217  0.01687796]\n",
      "total variance is: 0.964708692189\n",
      "square error for pca is 183722.737975\n"
     ]
    }
   ],
   "source": [
    "pca_variance(whole_data_scaled)\n",
    "pca_variance(whole_data[:,1:18])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should normalize the data, otherwise the square err would be extremely high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare NMF and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.67714157e+00   1.11567737e+00   4.47547919e-01   3.35132177e-03\n",
      "    4.59292704e-03   2.26217718e+00   2.93642713e-01   2.72600887e-02\n",
      "    3.17739562e-01   0.00000000e+00   0.00000000e+00   5.53743607e-02\n",
      "    6.15110221e-03   0.00000000e+00   4.66871602e-03   5.40358491e-04\n",
      "    2.23646912e-03]\n",
      " [  9.79698105e-02   5.84164627e-02   9.39180613e-03   3.19142949e-03\n",
      "    6.39028130e-03   0.00000000e+00   0.00000000e+00   1.51689196e+00\n",
      "    1.40225401e+00   3.07530145e-01   2.57060537e-01   3.55000002e-02\n",
      "    6.41198766e-02   1.38461604e-01   7.48537535e-03   1.45626858e-03\n",
      "    8.25034186e-03]\n",
      " [  0.00000000e+00   3.27229681e-02   5.28341883e-02   1.85422994e-03\n",
      "    5.57425552e-03   2.46000858e-01   2.49168906e-01   0.00000000e+00\n",
      "    5.44942479e-01   3.51137881e-01   2.87817891e-01   5.66606064e-02\n",
      "    8.70405189e-02   2.72599371e-01   9.50272260e-03   2.89953958e-03\n",
      "    6.47218688e-03]]\n",
      "the square_value for NMF is : 0.00170683946637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def nmf_vari(data):\n",
    "    model = NMF(n_components=3, init='random', random_state=0)\n",
    "    nmf=model.fit_transform(data)\n",
    "    ori_nmf=model.inverse_transform(nmf)\n",
    "    square_error=metrics.mean_squared_error(data,ori_nmf)\n",
    "    print model.components_\n",
    "    return square_error\n",
    "err=nmf_vari(whole_data_scaled)\n",
    "print \"the square_value for NMF is :\" ,err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The square err of NMF is 0.00170683946637, higher than the square err of PCA. The contribute of each feature to the 3 components is shown on the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " PCA has a better performance than NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
